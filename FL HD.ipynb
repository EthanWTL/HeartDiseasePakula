{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f37971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "cle_train = pd.read_csv('TrainTestData/cle_train.csv')\n",
    "cle_test = pd.read_csv('TrainTestData/cle_test.csv')\n",
    "    \n",
    "hun_train = pd.read_csv('TrainTestData/hun_train.csv')\n",
    "hun_test = pd.read_csv('TrainTestData/hun_test.csv')\n",
    "\n",
    "swi_train = pd.read_csv('TrainTestData/swi_train.csv')\n",
    "swi_test = pd.read_csv('TrainTestData/swi_test.csv')\n",
    "\n",
    "vir_train = pd.read_csv('TrainTestData/vir_train.csv')\n",
    "vir_test = pd.read_csv('TrainTestData/vir_test.csv')\n",
    "\n",
    "    \n",
    "cle_X_train = cle_train.iloc[:,:-1]\n",
    "cle_Y_train = cle_train.iloc[:,-1]\n",
    "\n",
    "cle_X_test = cle_test.iloc[:,:-1]\n",
    "cle_Y_test = cle_test.iloc[:,-1]\n",
    "\n",
    "cle_Y_train_binary = cle_Y_train.apply(lambda x: 1 if x > 0 else 0)\n",
    "cle_Y_test_binary = cle_Y_test.apply(lambda x: 1 if x > 0 else 0)\n",
    "#cle_Y_train_binary = lb.fit_transform(cle_Y_train_binary)\n",
    "#cle_Y_test_binary = lb.fit_transform(cle_Y_test_binary)\n",
    "\n",
    "hun_X_train = hun_train.iloc[:,:-1]\n",
    "hun_Y_train = hun_train.iloc[:,-1]\n",
    "hun_X_test = hun_test.iloc[:,:-1]\n",
    "hun_Y_test = hun_test.iloc[:,-1]\n",
    "\n",
    "hun_Y_train_binary = hun_Y_train.apply(lambda x: 1 if x > 0 else 0)\n",
    "hun_Y_test_binary = hun_Y_test.apply(lambda x: 1 if x > 0 else 0)\n",
    "#hun_Y_train_binary = lb.fit_transform(cle_Y_train_binary)\n",
    "#hun_Y_test_binary = lb.fit_transform(hun_Y_test_binary)\n",
    "\n",
    "swi_X_train = swi_train.iloc[:,:-1]\n",
    "swi_Y_train = swi_train.iloc[:,-1]\n",
    "\n",
    "swi_X_test = swi_test.iloc[:,:-1]\n",
    "swi_Y_test = swi_test.iloc[:,-1]\n",
    "\n",
    "swi_Y_train_binary = swi_Y_train.apply(lambda x: 1 if x > 0 else 0)\n",
    "swi_Y_test_binary = swi_Y_test.apply(lambda x: 1 if x > 0 else 0)\n",
    "#swi_Y_train_binary = lb.fit_transform(swi_Y_train_binary)\n",
    "#swi_Y_test_binary = lb.fit_transform(swi_Y_test_binary)\n",
    "\n",
    "vir_X_train = vir_train.iloc[:,:-1]\n",
    "vir_Y_train = vir_train.iloc[:,-1]\n",
    "\n",
    "vir_X_test = vir_test.iloc[:,:-1]\n",
    "vir_Y_test = vir_test.iloc[:,-1]\n",
    "\n",
    "vir_Y_train_binary = vir_Y_train.apply(lambda x: 1 if x > 0 else 0)\n",
    "vir_Y_test_binary = vir_Y_test.apply(lambda x: 1 if x > 0 else 0)\n",
    "#vir_Y_train_binary = lb.fit_transform(vir_Y_train_binary)\n",
    "#vir_Y_test_binary = lb.fit_transform(vir_Y_test_binary)\n",
    "\n",
    "\n",
    "X_test = pd.concat([cle_X_test,hun_X_test,swi_X_test,vir_X_test])\n",
    "y_test = pd.concat([cle_Y_test_binary,hun_Y_test_binary,swi_Y_test_binary,vir_Y_test_binary])\n",
    "\n",
    "X_train = pd.concat([cle_X_train,hun_X_train,swi_X_train,vir_X_train])\n",
    "y_train = pd.concat([cle_Y_train_binary,hun_Y_train_binary,swi_Y_train_binary,vir_Y_train_binary])\n",
    "\n",
    "def create_clients():\n",
    "    cle_zip = list(zip(cle_X_train.values,cle_Y_train_binary))\n",
    "    hun_zip = list(zip(hun_X_train.values,hun_Y_train_binary))\n",
    "    vir_zip = list(zip(vir_X_train.values,vir_Y_train_binary))\n",
    "    swi_zip = list(zip(swi_X_train.values,swi_Y_train_binary))\n",
    "    \n",
    "    shards = [cle_zip, hun_zip, vir_zip,swi_zip]\n",
    "    client_names = [\"client_1\",\"client_2\",\"client_3\",\"client_4\"]\n",
    "    dic = {client_names[i] : shards[i] for i in range(len(client_names))}\n",
    "    return dic\n",
    "\n",
    "\n",
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)\n",
    "\n",
    "\n",
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(200, input_shape=(shape,)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"sigmoid\"))\n",
    "        return model\n",
    "    \n",
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    length = len(y_test)\n",
    "    Y_test = tf.reshape(Y_test,(length,1))\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), Y_test)\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94fcc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\optimizers\\legacy\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 0 | global_acc: 70.036% | global_loss: 0.6857295632362366\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 1 | global_acc: 77.978% | global_loss: 0.6778998970985413\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 2 | global_acc: 79.422% | global_loss: 0.670996904373169\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 3 | global_acc: 80.144% | global_loss: 0.6648367047309875\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 4 | global_acc: 80.505% | global_loss: 0.6589645147323608\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 5 | global_acc: 81.227% | global_loss: 0.6522791981697083\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 6 | global_acc: 82.310% | global_loss: 0.6466462016105652\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 7 | global_acc: 81.949% | global_loss: 0.640278697013855\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 8 | global_acc: 82.310% | global_loss: 0.6343693137168884\n",
      "9/9 [==============================] - 0s 500us/step\n",
      "comm_round: 9 | global_acc: 81.949% | global_loss: 0.628706157207489\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 10 | global_acc: 81.949% | global_loss: 0.6234225630760193\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 11 | global_acc: 81.949% | global_loss: 0.6178537607192993\n",
      "9/9 [==============================] - 0s 500us/step\n",
      "comm_round: 12 | global_acc: 81.949% | global_loss: 0.6133363246917725\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 13 | global_acc: 82.310% | global_loss: 0.6085638999938965\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 14 | global_acc: 82.671% | global_loss: 0.6042201519012451\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 15 | global_acc: 83.032% | global_loss: 0.5998590588569641\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 16 | global_acc: 83.394% | global_loss: 0.5955491065979004\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 17 | global_acc: 83.032% | global_loss: 0.591979444026947\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 18 | global_acc: 83.755% | global_loss: 0.5891073346138\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 19 | global_acc: 83.755% | global_loss: 0.585956871509552\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 20 | global_acc: 83.394% | global_loss: 0.5833150148391724\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 21 | global_acc: 83.394% | global_loss: 0.581104576587677\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 22 | global_acc: 84.116% | global_loss: 0.577972948551178\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 23 | global_acc: 84.116% | global_loss: 0.5756537318229675\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 24 | global_acc: 84.116% | global_loss: 0.5735453963279724\n",
      "9/9 [==============================] - 0s 500us/step\n",
      "comm_round: 25 | global_acc: 83.755% | global_loss: 0.5720351934432983\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 26 | global_acc: 84.116% | global_loss: 0.5700171589851379\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 27 | global_acc: 84.116% | global_loss: 0.5683810710906982\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 28 | global_acc: 84.116% | global_loss: 0.5666046738624573\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 29 | global_acc: 84.116% | global_loss: 0.5650246739387512\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 30 | global_acc: 84.116% | global_loss: 0.5638613700866699\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 31 | global_acc: 84.477% | global_loss: 0.5620419979095459\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 32 | global_acc: 84.116% | global_loss: 0.5616234540939331\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 33 | global_acc: 84.477% | global_loss: 0.5603373050689697\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 34 | global_acc: 84.838% | global_loss: 0.5593753457069397\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 35 | global_acc: 85.199% | global_loss: 0.5584050416946411\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 36 | global_acc: 84.477% | global_loss: 0.556724488735199\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 37 | global_acc: 84.838% | global_loss: 0.5565996766090393\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 38 | global_acc: 85.199% | global_loss: 0.5553156733512878\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 39 | global_acc: 85.199% | global_loss: 0.5546992421150208\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 40 | global_acc: 85.199% | global_loss: 0.5541174411773682\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 41 | global_acc: 84.838% | global_loss: 0.5531392693519592\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 42 | global_acc: 84.838% | global_loss: 0.5522496104240417\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 43 | global_acc: 85.199% | global_loss: 0.5522621870040894\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 44 | global_acc: 85.199% | global_loss: 0.5514604449272156\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 45 | global_acc: 84.838% | global_loss: 0.5510222315788269\n",
      "9/9 [==============================] - 0s 500us/step\n",
      "comm_round: 46 | global_acc: 84.838% | global_loss: 0.5504835247993469\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 47 | global_acc: 85.199% | global_loss: 0.5498042702674866\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 48 | global_acc: 84.838% | global_loss: 0.5492993593215942\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 49 | global_acc: 85.199% | global_loss: 0.5481629967689514\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 50 | global_acc: 85.199% | global_loss: 0.5480143427848816\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 51 | global_acc: 85.199% | global_loss: 0.5477527976036072\n",
      "9/9 [==============================] - 0s 500us/step\n",
      "comm_round: 52 | global_acc: 85.199% | global_loss: 0.5470358729362488\n",
      "9/9 [==============================] - 0s 500us/step\n",
      "comm_round: 53 | global_acc: 85.199% | global_loss: 0.5463867783546448\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 54 | global_acc: 85.199% | global_loss: 0.5458688139915466\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 55 | global_acc: 85.199% | global_loss: 0.5455979704856873\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 56 | global_acc: 85.199% | global_loss: 0.5446580648422241\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 57 | global_acc: 84.838% | global_loss: 0.5442236065864563\n",
      "9/9 [==============================] - 0s 500us/step\n",
      "comm_round: 58 | global_acc: 84.838% | global_loss: 0.5436913371086121\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 59 | global_acc: 84.838% | global_loss: 0.5434784889221191\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 60 | global_acc: 84.838% | global_loss: 0.5429051518440247\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 61 | global_acc: 84.838% | global_loss: 0.5424600839614868\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 62 | global_acc: 84.838% | global_loss: 0.5418341159820557\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 63 | global_acc: 84.838% | global_loss: 0.5412819981575012\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 64 | global_acc: 84.838% | global_loss: 0.5404881834983826\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 65 | global_acc: 84.838% | global_loss: 0.5401518940925598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 66 | global_acc: 84.838% | global_loss: 0.5398163795471191\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 67 | global_acc: 84.838% | global_loss: 0.5394105315208435\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 68 | global_acc: 84.838% | global_loss: 0.5391732454299927\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 69 | global_acc: 84.838% | global_loss: 0.5385004281997681\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 70 | global_acc: 84.477% | global_loss: 0.538516104221344\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 71 | global_acc: 84.838% | global_loss: 0.5381065011024475\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 72 | global_acc: 84.838% | global_loss: 0.537407636642456\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 73 | global_acc: 84.838% | global_loss: 0.5372222661972046\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 74 | global_acc: 84.477% | global_loss: 0.5370427370071411\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 75 | global_acc: 84.838% | global_loss: 0.5364744663238525\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 76 | global_acc: 84.838% | global_loss: 0.5363735556602478\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 77 | global_acc: 84.838% | global_loss: 0.5359712243080139\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 78 | global_acc: 84.838% | global_loss: 0.5358617305755615\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 79 | global_acc: 85.199% | global_loss: 0.5355038046836853\n",
      "9/9 [==============================] - 0s 1000us/step\n",
      "comm_round: 80 | global_acc: 84.838% | global_loss: 0.535484790802002\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 81 | global_acc: 85.199% | global_loss: 0.5350618362426758\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 82 | global_acc: 85.199% | global_loss: 0.5348312854766846\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 83 | global_acc: 85.199% | global_loss: 0.5347675085067749\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 84 | global_acc: 85.199% | global_loss: 0.534309983253479\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 85 | global_acc: 85.199% | global_loss: 0.5339961051940918\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 86 | global_acc: 85.199% | global_loss: 0.5341747999191284\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 87 | global_acc: 85.199% | global_loss: 0.5338265895843506\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 88 | global_acc: 85.199% | global_loss: 0.5336120128631592\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 89 | global_acc: 85.199% | global_loss: 0.5336734056472778\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 90 | global_acc: 85.199% | global_loss: 0.5332657694816589\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 91 | global_acc: 85.199% | global_loss: 0.5328145623207092\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 92 | global_acc: 85.199% | global_loss: 0.5323665142059326\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 93 | global_acc: 85.199% | global_loss: 0.5316907167434692\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 94 | global_acc: 85.199% | global_loss: 0.5313637256622314\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 95 | global_acc: 85.199% | global_loss: 0.530920147895813\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 96 | global_acc: 85.199% | global_loss: 0.5311411619186401\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 97 | global_acc: 85.199% | global_loss: 0.5307590365409851\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 98 | global_acc: 85.199% | global_loss: 0.5305699706077576\n",
      "9/9 [==============================] - 0s 625us/step\n",
      "comm_round: 99 | global_acc: 85.560% | global_loss: 0.5299866795539856\n",
      "9/9 [==============================] - 0s 750us/step\n",
      "comm_round: 1 | global_acc: 84.838% | global_loss: 0.5422181487083435\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "#create clients\n",
    "clients = create_clients()\n",
    "\n",
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
    "\n",
    "comms_round = 100\n",
    "    \n",
    "#create optimizer\n",
    "lr = 0.01 \n",
    "loss='sparse_categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(lr=lr, decay=lr / comms_round, momentum=0.9) \n",
    "\n",
    "#initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(22, 2)\n",
    "        \n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(22, 2)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "        SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)\n",
    "        smlp_SGD = SimpleMLP()\n",
    "        SGD_model = smlp_SGD.build(22, 2) \n",
    "\n",
    "        SGD_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# fit the SGD training data to model\n",
    "_ = SGD_model.fit(SGD_dataset, epochs=100, verbose=0)\n",
    "\n",
    "#test the SGD global model and print out metrics\n",
    "for(X_test, Y_test) in test_batched:\n",
    "        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad33fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b13ff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 625us/step\n"
     ]
    }
   ],
   "source": [
    "Y_predictions = np.argmax(SGD_model.predict(X_test),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bc551a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[102,  21],\n",
       "       [ 21, 133]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(Y_predictions, Y_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a83e0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83       123\n",
      "           1       0.86      0.86      0.86       154\n",
      "\n",
      "    accuracy                           0.85       277\n",
      "   macro avg       0.85      0.85      0.85       277\n",
      "weighted avg       0.85      0.85      0.85       277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163a424b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mSGD_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFL_HD_ANN_85\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\GPU\\lib\\site-packages\\h5py\\_hl\\group.py:161\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    158\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    159\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 161\u001b[0m dsid \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmake_new_dset(group, shape, dtype, data, name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    162\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\GPU\\lib\\site-packages\\h5py\\_hl\\dataset.py:156\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     sid \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[1;32m--> 156\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m \u001b[43mh5d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m    159\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5d.pyx:87\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create dataset (name already exists)"
     ]
    }
   ],
   "source": [
    "SGD_model.save('FL_HD_ANN_85%.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4c637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
